{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d1fba0-593e-4ce5-82c5-375c007e1e63",
   "metadata": {},
   "source": [
    "# UR5 Manipulator Sensor Data — Notebook 05: Target Definition & Baseline Modeling\n",
    "\n",
    "Objective: This final notebook in the EDA repository concludes the data preparation phase by:\n",
    "\n",
    "1. Defining the Target Variable (Y): Using insights from the EDA (Notebook 04) to create a definitive, labeled ANOMALY_FLAG.\n",
    "\n",
    "2. Exporting the combined X and Y matrix for immediate use in another repository.\n",
    "\n",
    "3. Final Data Split: Separating the feature matrix (X) from the target vector (Y) and splitting them into standard Train/Test sets.\n",
    "\n",
    "4. Establishing a Baseline: Training a simple model (Dummy and Logistic Regression) to set a performance benchmark for the future ML repository.\n",
    "\n",
    "5. Final Save: Exporting the prepared, split datasets, completing the EDA project.\n",
    "\n",
    "**Input Data:**\n",
    "\n",
    "- Feature Set: `../data/features/feature_set.parquet` (≈153k rows, ≈250 features).\n",
    "\n",
    "**Output:**\n",
    "\n",
    "- Full ML-Ready Dataset: `../data/ml_ready/full_ml_ready_data.parquet` (X+Y combined).\n",
    "    \n",
    "- Prepared ML Files: Xtrain​,Xtest​,Ytrain​,Ytest​ saved to `../data/ml_ready`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea08a06-7009-45a6-98e8-004ccf74176b",
   "metadata": {},
   "source": [
    "## Step 1. Setup and Data Loading\n",
    "\n",
    "We load the necessary libraries and the fully engineered feature set from previous step. We also introduce the ML specific libraries for splitting and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d018be1-1c06-4c33-8264-9b24fcb29a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "# Define paths\n",
    "feature_set_path = \"../data/features/feature_set.parquet\"\n",
    "output_dir = \"../data/ml_ready\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a61f5dc-1fd5-466e-8625-cde883d2ded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Feature set loaded successfully. Shape: (153658, 127)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROBOT_TIME</th>\n",
       "      <th>ROBOT_TARGET_JOINT_POSITIONS (J1)</th>\n",
       "      <th>ROBOT_TARGET_JOINT_POSITIONS (J2)</th>\n",
       "      <th>ROBOT_TARGET_JOINT_POSITIONS (J3)</th>\n",
       "      <th>ROBOT_TARGET_JOINT_POSITIONS (J4)</th>\n",
       "      <th>ROBOT_TARGET_JOINT_POSITIONS (J5)</th>\n",
       "      <th>ROBOT_TARGET_JOINT_POSITIONS (J6)</th>\n",
       "      <th>ROBOT_ACTUAL_JOINT_POSITIONS (J1)</th>\n",
       "      <th>ROBOT_ACTUAL_JOINT_POSITIONS (J2)</th>\n",
       "      <th>ROBOT_ACTUAL_JOINT_POSITIONS (J3)</th>\n",
       "      <th>...</th>\n",
       "      <th>ROBOT_JOINT_CONTROL_CURRENT_J1_ROLL_MEAN_50</th>\n",
       "      <th>ROBOT_JOINT_CONTROL_CURRENT_J1_ROLL_STD_50</th>\n",
       "      <th>ROBOT_ACTUAL_JOINT_VELOCITIES_J1_ROLL_MEAN_50</th>\n",
       "      <th>ROBOT_ACTUAL_JOINT_VELOCITIES_J1_ROLL_STD_50</th>\n",
       "      <th>ROBOT_TCP_FORCE_x_ROLL_MEAN_50</th>\n",
       "      <th>ROBOT_TCP_FORCE_x_ROLL_STD_50</th>\n",
       "      <th>ROBOT_TCP_FORCE_z_ROLL_MEAN_50</th>\n",
       "      <th>ROBOT_TCP_FORCE_z_ROLL_STD_50</th>\n",
       "      <th>ERROR_JOINT_POSITIONS_J1_ROLL_MEAN_50</th>\n",
       "      <th>ERROR_JOINT_POSITIONS_J1_ROLL_STD_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>747.248</td>\n",
       "      <td>-26.880069</td>\n",
       "      <td>-79.911609</td>\n",
       "      <td>57.095392</td>\n",
       "      <td>-157.771764</td>\n",
       "      <td>-105.009613</td>\n",
       "      <td>-44.724779</td>\n",
       "      <td>-26.87662</td>\n",
       "      <td>-79.910908</td>\n",
       "      <td>57.096775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-26.387519</td>\n",
       "      <td>0.60378</td>\n",
       "      <td>7.837572</td>\n",
       "      <td>0.65046</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>747.256</td>\n",
       "      <td>-26.880069</td>\n",
       "      <td>-79.911609</td>\n",
       "      <td>57.095392</td>\n",
       "      <td>-157.771764</td>\n",
       "      <td>-105.009613</td>\n",
       "      <td>-44.724779</td>\n",
       "      <td>-26.87662</td>\n",
       "      <td>-79.910225</td>\n",
       "      <td>57.096092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-26.387519</td>\n",
       "      <td>0.60378</td>\n",
       "      <td>7.837572</td>\n",
       "      <td>0.65046</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROBOT_TIME  ROBOT_TARGET_JOINT_POSITIONS (J1)  \\\n",
       "0     747.248                         -26.880069   \n",
       "1     747.256                         -26.880069   \n",
       "\n",
       "   ROBOT_TARGET_JOINT_POSITIONS (J2)  ROBOT_TARGET_JOINT_POSITIONS (J3)  \\\n",
       "0                         -79.911609                          57.095392   \n",
       "1                         -79.911609                          57.095392   \n",
       "\n",
       "   ROBOT_TARGET_JOINT_POSITIONS (J4)  ROBOT_TARGET_JOINT_POSITIONS (J5)  \\\n",
       "0                        -157.771764                        -105.009613   \n",
       "1                        -157.771764                        -105.009613   \n",
       "\n",
       "   ROBOT_TARGET_JOINT_POSITIONS (J6)  ROBOT_ACTUAL_JOINT_POSITIONS (J1)  \\\n",
       "0                         -44.724779                          -26.87662   \n",
       "1                         -44.724779                          -26.87662   \n",
       "\n",
       "   ROBOT_ACTUAL_JOINT_POSITIONS (J2)  ROBOT_ACTUAL_JOINT_POSITIONS (J3)  ...  \\\n",
       "0                         -79.910908                          57.096775  ...   \n",
       "1                         -79.910225                          57.096092  ...   \n",
       "\n",
       "   ROBOT_JOINT_CONTROL_CURRENT_J1_ROLL_MEAN_50  \\\n",
       "0                                     0.237228   \n",
       "1                                     0.237228   \n",
       "\n",
       "   ROBOT_JOINT_CONTROL_CURRENT_J1_ROLL_STD_50  \\\n",
       "0                                    0.008602   \n",
       "1                                    0.008602   \n",
       "\n",
       "   ROBOT_ACTUAL_JOINT_VELOCITIES_J1_ROLL_MEAN_50  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "\n",
       "   ROBOT_ACTUAL_JOINT_VELOCITIES_J1_ROLL_STD_50  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "\n",
       "   ROBOT_TCP_FORCE_x_ROLL_MEAN_50  ROBOT_TCP_FORCE_x_ROLL_STD_50  \\\n",
       "0                      -26.387519                        0.60378   \n",
       "1                      -26.387519                        0.60378   \n",
       "\n",
       "   ROBOT_TCP_FORCE_z_ROLL_MEAN_50  ROBOT_TCP_FORCE_z_ROLL_STD_50  \\\n",
       "0                        7.837572                        0.65046   \n",
       "1                        7.837572                        0.65046   \n",
       "\n",
       "   ERROR_JOINT_POSITIONS_J1_ROLL_MEAN_50  ERROR_JOINT_POSITIONS_J1_ROLL_STD_50  \n",
       "0                               0.002732                              0.001374  \n",
       "1                               0.002732                              0.001374  \n",
       "\n",
       "[2 rows x 127 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the feature set\n",
    "try:\n",
    "    df = pd.read_parquet(feature_set_path)\n",
    "    print(f\"✔ Feature set loaded successfully. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {feature_set_path}. Please run Notebook 03 first.\")\n",
    "\n",
    "# Drop the plotting index if it exists\n",
    "df = df.drop(columns=['TIME_INDEX'], errors='ignore')\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10861bfe-0244-43be-8cb9-6bb4454fd256",
   "metadata": {},
   "source": [
    "## Step 2. Target Definition: Creating the Anomaly Flag (Y)\n",
    "\n",
    "Based on the temporal and distributional analysis in Notebook 04, the positional error for Joint 1 (ERROR_JOINT_POSITIONS_(J1)) provides the clearest signal of a mechanical anomaly.\n",
    "\n",
    "We define an ANOMALY_FLAG (Y) using a conservative threshold of 0.5 (or the 99.9th percentile) to isolate only the most severe deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c605a9-7963-400c-a731-c96c69f68450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 99.99th Percentile Threshold: 0.081011\n",
      "\n",
      "--- Recalculated Target Summary ---\n",
      "Total Anomalous Records (Y=1): 16\n",
      "Total Records: 153658\n",
      "Class Imbalance: 0.0104%\n",
      "\n",
      "Interpretation: The target class is now correctly defined with a minimal number of positive examples.\n",
      "The primary challenge for the ML phase remains achieving high Recall on this rare class.\n"
     ]
    }
   ],
   "source": [
    "# Select the positional error feature for Joint 1\n",
    "error_col = 'ERROR_JOINT_POSITIONS_(J1)'\n",
    "\n",
    "# 1. CALCULATE DATA-DRIVEN THRESHOLD\n",
    "# Use the 99.99th percentile of the error column to ensure we capture the most extreme cases.\n",
    "# This makes the definition of \"anomaly\" relative to the dataset's own performance.\n",
    "percentile_threshold = df[error_col].quantile(0.9999)\n",
    "\n",
    "# We will use this calculated threshold\n",
    "ANOMALY_THRESHOLD = percentile_threshold\n",
    "print(f\"Calculated 99.99th Percentile Threshold: {ANOMALY_THRESHOLD:.6f}\")\n",
    "\n",
    "\n",
    "# 2. Create the binary target variable (Y)\n",
    "# Use the calculated threshold for labeling\n",
    "df['ANOMALY_FLAG'] = np.where(df[error_col] >= ANOMALY_THRESHOLD, 1, 0)\n",
    "\n",
    "# Drop all positional error features to prevent data leakage.\n",
    "# NOTE: Dropping the column used to define the target is CRITICAL.\n",
    "error_cols_to_drop = [col for col in df.columns if 'ERROR_JOINT_POSITIONS' in col]\n",
    "df = df.drop(columns=error_cols_to_drop)\n",
    "\n",
    "# Calculate class imbalance\n",
    "anomaly_count = df['ANOMALY_FLAG'].sum()\n",
    "total_count = len(df)\n",
    "imbalance = (anomaly_count / total_count) * 100\n",
    "\n",
    "print(f\"\\n--- Recalculated Target Summary ---\")\n",
    "print(f\"Total Anomalous Records (Y=1): {anomaly_count}\")\n",
    "print(f\"Total Records: {total_count}\")\n",
    "print(f\"Class Imbalance: {imbalance:.4f}%\")\n",
    "\n",
    "# \n",
    "\n",
    "print(\"\\nInterpretation: The target class is now correctly defined with a minimal number of positive examples.\")\n",
    "print(\"The primary challenge for the ML phase remains achieving high Recall on this rare class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11843856-e9f9-414d-8a0d-d346ab66fef0",
   "metadata": {},
   "source": [
    "## Step 3. Export Combined X+Y Matrix\n",
    "\n",
    "We save the complete, labeled dataset (X and Y combined) for convenience in the future ML repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07a58c9-63b2-42e2-afe0-191e09bb26ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Full ML-ready dataset saved at: ../data/ml_ready/full_ml_ready_data.parquet\n"
     ]
    }
   ],
   "source": [
    "full_path = os.path.join(output_dir, \"full_ml_ready_data.parquet\")\n",
    "df.to_parquet(full_path, index=False)\n",
    "print(f\"✔ Full ML-ready dataset saved at: {full_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee02af1-9ae7-47a2-8de5-6b3d353d9f4d",
   "metadata": {},
   "source": [
    "## Step 4. Final Data Split: Time-Aware Train/Test Sets\n",
    "\n",
    "Since the data is a time series, the split must be chronological (time-aware). We reserve the last 20% of the operational window for the test set, mimicking how a model trained on past data would perform on unseen, future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c01d9c-682e-41d2-849e-3c9bc1b4f11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split point set at index: 77096 to reserve 4 anomalies for testing.\n",
      "\n",
      "--- Data Split Summary (INDEX-BASED) ---\n",
      "X_train shape: (77097, 119)\n",
      "X_test shape: (76561, 119)\n",
      "Y_train Anomaly Count: 13\n",
      "Y_test Anomaly Count: 3\n",
      "Y_train Anomaly Rate: 0.0002\n",
      "Y_test Anomaly Rate: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target (Y)\n",
    "X = df.drop(columns=['ANOMALY_FLAG'])\n",
    "Y = df['ANOMALY_FLAG']\n",
    "\n",
    "# --- Step 1: Find the Index for a Guaranteed Split ---\n",
    "# We want to reserve the last 4 anomalies for the test set.\n",
    "\n",
    "# 1. Get the indices (row numbers) where an anomaly (Y=1) occurred.\n",
    "anomaly_indices = Y[Y == 1].index\n",
    "\n",
    "# 2. Convert the Index object to a list for reliable positional indexing \n",
    "anomaly_indices_list = anomaly_indices.to_list()\n",
    "\n",
    "# 3. Determine the split point: the index of the 4th to last anomaly.\n",
    "if len(anomaly_indices_list) >= 4:\n",
    "    # Use standard list indexing to get the element at the 4th from last position\n",
    "    split_index = anomaly_indices_list[-4]\n",
    "    print(f\"Split point set at index: {split_index} to reserve 4 anomalies for testing.\")\n",
    "else:\n",
    "    # Fallback (should not happen since we know we have 16)\n",
    "    split_index = anomaly_indices_list[0] if len(anomaly_indices_list) > 0 else int(len(df) * 0.9)\n",
    "    print(\"Warning: Fewer than 4 anomalies found. Splitting at 90% index.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Perform the Index-Based Split ---\n",
    "# Use the found index to slice the DataFrames.\n",
    "# X_train and Y_train include the row at the split_index.\n",
    "X_train = X.loc[:split_index]\n",
    "Y_train = Y.loc[:split_index]\n",
    "\n",
    "# X_test and Y_test start immediately after the split index.\n",
    "X_test = X.loc[split_index + 1:]\n",
    "Y_test = Y.loc[split_index + 1:]\n",
    "\n",
    "\n",
    "print(\"\\n--- Data Split Summary (INDEX-BASED) ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_train Anomaly Count: {Y_train.sum()}\")\n",
    "print(f\"Y_test Anomaly Count: {Y_test.sum()}\")\n",
    "print(f\"Y_train Anomaly Rate: {Y_train.mean():.4f}\")\n",
    "print(f\"Y_test Anomaly Rate: {Y_test.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e158dd-2b0f-4d19-95ef-ef8585d63d85",
   "metadata": {},
   "source": [
    "## Step 5. Baseline Modeling and Evaluation\n",
    "\n",
    "We establish a concrete baseline to set the minimum performance bar for the future ML models. Given the extreme imbalance, we focus on Recall (catching anomalies) and F1-Score (balancing precision and recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62f45d-cda8-4999-8090-8a62973a09f0",
   "metadata": {},
   "source": [
    "### 5.1 Dummy Classifier Baseline (The Minimum Bar)\n",
    "\n",
    "The dummy classifier shows what happens when we simply guess the majority class every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e91a63-b84b-4c4b-89ae-2c020b817aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dummy Model Baseline (Absolute Minimum Bar) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     76558\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00     76561\n",
      "   macro avg       0.50      0.50      0.50     76561\n",
      "weighted avg       1.00      1.00      1.00     76561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dummy Classifier (Predicts the majority class: 0)\n",
    "dummy_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_model.fit(X_train, Y_train)\n",
    "dummy_preds = dummy_model.predict(X_test)\n",
    "\n",
    "print(\"--- Dummy Model Baseline (Absolute Minimum Bar) ---\")\n",
    "print(classification_report(Y_test, dummy_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743f2d4-06e1-4c2d-8d61-667c607407a7",
   "metadata": {},
   "source": [
    "**Conclusion**: The Dummy Model, which simply predicts \"Normal\" (Y=0) every time, fails entirely at the task of anomaly detection, as expected. Your future machine learning models must achieve an F1-Score greater than 0.00 to be considered useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb7ddd-a93f-4132-9d44-b5b34df1a216",
   "metadata": {},
   "source": [
    "### 5.2 Logistic Regression Baseline\n",
    "\n",
    "Logistic Regression, a simple linear model, provides a benchmark that incorporates the actual features. We use class_weight='balanced' to try and force the model to pay attention to the rare anomaly class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff181d2b-d4ae-43d5-8dba-ff2f2caaf3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression Baseline ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     76558\n",
      "           1       0.00      0.67      0.00         3\n",
      "\n",
      "    accuracy                           0.98     76561\n",
      "   macro avg       0.50      0.82      0.50     76561\n",
      "weighted avg       1.00      0.98      0.99     76561\n",
      "\n",
      "\n",
      "Target Class (Y=1) Metrics:\n",
      "Baseline F1-Score: 0.0029\n",
      "Baseline Recall: 0.6667 (Ability to catch true faults)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression (Simple Learning Bar)\n",
    "# class_weight='balanced' is essential for imbalanced data\n",
    "log_model = LogisticRegression(solver='liblinear', random_state=42, max_iter=200, class_weight='balanced')\n",
    "log_model.fit(X_train, Y_train)\n",
    "log_preds = log_model.predict(X_test)\n",
    "\n",
    "print(\"--- Logistic Regression Baseline ---\")\n",
    "print(classification_report(Y_test, log_preds, zero_division=0))\n",
    "\n",
    "# Report the key metric\n",
    "baseline_f1 = f1_score(Y_test, log_preds)\n",
    "baseline_recall = recall_score(Y_test, log_preds)\n",
    "\n",
    "print(f\"\\nTarget Class (Y=1) Metrics:\")\n",
    "print(f\"Baseline F1-Score: {baseline_f1:.4f}\")\n",
    "print(f\"Baseline Recall: {baseline_recall:.4f} (Ability to catch true faults)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b928d5-f214-4bdc-8b96-c4b65630548b",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "\n",
    "The Logistic Regression model, by using class_weight='balanced', prioritized Recall (catching faults) but paid a heavy price in Precision (generating false alarms).\n",
    "\n",
    "The goal for your next, dedicated ML repository is now clearly defined: Build a model that achieves a Recall of >0.67 while simultaneously dramatically improving the Precision (and thus the F1-Score) from 0.0029."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704dfe2c-c1cf-4df9-a51d-fbe220fc3806",
   "metadata": {},
   "source": [
    "## Step 6. Final Save: Exporting Split Datasets\n",
    "\n",
    "The final step is to save the four split files, completing the Manipulator Health Monitoring EDA project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c6fc55-3d22-4bbe-9c0b-4d0f639187fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ Final ML-Ready datasets (X_train, X_test, Y_train, Y_test) saved successfully.\n",
      "All files exported to: ../data/ml_ready\n"
     ]
    }
   ],
   "source": [
    "# --- Save Split Datasets ---\n",
    "ml_ready_dir = \"../data/ml_ready\"\n",
    "\n",
    "# Save X (Features)\n",
    "X_train.to_parquet(os.path.join(ml_ready_dir, \"X_train.parquet\"), index=False)\n",
    "X_test.to_parquet(os.path.join(ml_ready_dir, \"X_test.parquet\"), index=False)\n",
    "\n",
    "# Save Y (Target)\n",
    "Y_train.to_frame(name='ANOMALY_FLAG').to_parquet(os.path.join(ml_ready_dir, \"Y_train.parquet\"), index=False)\n",
    "Y_test.to_frame(name='ANOMALY_FLAG').to_parquet(os.path.join(ml_ready_dir, \"Y_test.parquet\"), index=False)\n",
    "\n",
    "\n",
    "print(\"\\n✔ Final ML-Ready datasets (X_train, X_test, Y_train, Y_test) saved successfully.\")\n",
    "print(f\"All files exported to: {ml_ready_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7020c443-0a19-4897-8ad5-9257423d87c7",
   "metadata": {},
   "source": [
    "# Conclusion of the EDA Project Repository\n",
    "\n",
    "The Manipulator Health Monitoring EDA repository is now complete. We successfully executed the end-to-end data pipeline:\n",
    "\n",
    "1. **01_data_exploration**: Defined structure and parsed non-standard files.\n",
    "\n",
    "2. **02_data_cleaning**: Cleaned strings, imputed NaN values, and converted all data to float64.\n",
    "\n",
    "3. **03_feature_engineering**: Created advanced time-series features (Lagging, Rolling Stats) and domain-specific error metrics.\n",
    "\n",
    "4. **04_deep_eda**: Validated features, identified severe class imbalance, and isolated fault events.\n",
    "\n",
    "5. **05_target_definition_and_baseline_modeling**: Defined the ANOMALY_FLAG target and established a measurable baseline F1-Score for future models.\n",
    "\n",
    "The project is now ready to transition to the dedicated ML Modeling Repository where advanced techniques (e.g., XGBoost, LSTM, Sampling methods, and Hyperparameter Tuning) will be employed to surpass the established baseline performance.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry: Manipulator-Health-Monitoring-EDA)",
   "language": "python",
   "name": "manipulator-health-monitoring-eda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
